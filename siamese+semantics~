from __future__ import print_function
import os
import numpy as np
np.random.seed(1337)
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
# fix random seed for reproducibility
np.random.seed(7)
from keras.layers import merge
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
import sys
import csv
import nltk
from nltk import pos_tag
from nltk.corpus import stopwords 
from scipy.spatial.distance import cosine as d
import re
from nltk.corpus import wordnet
from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import xgboost




s_words=stopwords.words('english')
top_words_considered=10000
max_sentence_length=10
validation_split=0.4
file1='input/train.csv'
file2='input/test.csv'
word_vector_file='glove.6B.300d.txt'
print ("Loading word vectors.....")
embeddings_index = {}
f = open(word_vector_file)
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs



f.close()
print ("Word Vectors Loaded!!")
print('Found %s word vectors.' % len(embeddings_index))
word_vector_length=len(embeddings_index.get('dog'))
print ("Loading the data file....")
f=open(file1)
f=csv.reader(f)
train_texts1=[]
train_texts2=[]
x=[]
train_y=[]
for i in f:
  try:
    x.append(str(i[3]).encode('utf-8'))
    x.append(str(i[4]).encode('utf-8'))
    x.append(int(i[5]))
    train_texts1.append(str(i[3]).encode('utf-8'))
    train_texts2.append(str(i[4]).encode('utf-8'))
    train_y.append(int(i[5]))
  except:
    pass




x1=[]
for j in train_texts1:
  s=[] 
  for i in j.lower().split():
    if ((re.sub('[^a-zA-Z0-9]','',i) not in s)):
      s.append(re.sub('[^a-zA-Z0-9]','',i))
  x1.append(' '.join(s))




x2=[]
for j in train_texts2:
  s=[] 
  for i in j.lower().split():
    if ((re.sub('[^a-zA-Z0-9]','',i) not in s)):
      s.append(re.sub('[^a-zA-Z0-9]','',i))
  x2.append(' '.join(s))



nn_x1=[]
adj_x1=[]
vbz_x1=[]
for i in x1:
  s1=[]
  s2=[]
  s3=[]
  for j in pos_tag(i.split()):
    if ('NN' in j[1]):
      s1.append(j[0])
    if ('JJ' in j[1]):
      s2.append(j[0])
    if ('VB' in j[1]):
      s3.append(j[0])
  nn_x1.append(' '.join(s1))
  adj_x1.append(' '.join(s2))
  vbz_x1.append(' '.join(s3))




nn_x2=[]
adj_x2=[]
vbz_x2=[]
for i in x2:
  s1=[]
  s2=[]
  s3=[]
  for j in pos_tag(i.split()):
    if ('NN' in j[1]):
      s1.append(j[0])
    if ('JJ' in j[1]):
      s2.append(j[0])
    if ('VB' in j[1]):
      s3.append(j[0])
  nn_x2.append(' '.join(s1))
  adj_x2.append(' '.join(s2))
  vbz_x2.append(' '.join(s3))
 




nn_match=[]
for i in np.arange(len(nn_x1)):
  if (len(np.unique((nn_x2[i]+' '+nn_x1[i]).split()))>0):
    nn_match.append(1.0*len(set(nn_x1[i].split()).intersection(nn_x2[i].split()))/len(np.unique((nn_x2[i]+' '+nn_x1[i]).split())))
  else:
    nn_match.append(1.0*0)



adj_match=[]
for i in np.arange(len(adj_x1)):
  if (len(np.unique((adj_x2[i]+' '+adj_x1[i]).split()))>0):
    adj_match.append(1.0*len(set(adj_x1[i].split()).intersection(adj_x2[i].split()))/len(np.unique((adj_x2[i]+' '+adj_x1[i]).split())))
  else:
    adj_match.append(1.0*0)



vbz_match=[]
for i in np.arange(len(vbz_x1)):
  if (len(np.unique((vbz_x2[i]+' '+vbz_x1[i]).split()))>0):
    vbz_match.append(1.0*len(set(vbz_x1[i].split()).intersection(vbz_x2[i].split()))/len(np.unique((vbz_x2[i]+' '+vbz_x1[i]).split())))
  else:
    vbz_match.append(1.0*0)




nn_semantic=[]
for i in np.arange(len(nn_x1)):
  s=[]
  for j in nn_x1[i].split():
    for k in nn_x2[i].split():
      try:
        s.append(1-d(embeddings_index.get(j),embeddings_index.get(k)))
      except:
        s.append(1.0*0)
  if (len(s)>0):
    nn_semantic.append(np.percentile(s,80))
  else:
    nn_semantic.append(0)




adj_semantic=[]
for i in np.arange(len(adj_x1)):
  s=[]
  for j in adj_x1[i].split():
    for k in adj_x2[i].split():
      try:
        s.append(1-d(embeddings_index.get(j),embeddings_index.get(k)))
      except:
        s.append(1.0*0)
  if (len(s)>0):
    adj_semantic.append(np.percentile(s,80))
  else:
    adj_semantic.append(0)




vbz_semantic=[]
for i in np.arange(len(vbz_x1)):
  s=[]
  for j in vbz_x1[i].split():
    for k in vbz_x2[i].split():
      try:
        s.append(1-d(embeddings_index.get(j),embeddings_index.get(k)))
      except:
        s.append(1.0*0)
  if (len(s)>0):
    vbz_semantic.append(np.percentile(s,80))
  else:
    vbz_semantic.append(0)





def syn_sim(w1,w2):
  s=[]
  try:
    for i in wordnet.synsets(w1):
      for j in wordnet.synsets(w2):
        s.append(i.wup_similarity(j))
  except:
    s.append(0)
  if (len(s)>0):
    return (np.max(s))
  else:
    return (0.0)
  




test_texts1=[]
test_texts2=[]
f=open(file2)
f=csv.reader(f)
x=[]
for i in f:
  try:
    x.append(str(i[1]).encode('utf-8'))
    x.append(str(i[2]).encode('utf-8'))
    test_texts1.append(str(i[1]).encode('utf-8'))
    test_texts2.append(str(i[2]).encode('utf-8'))
  except:
    pass
 




test_texts1=test_texts1[1:]
test_texts2=test_texts2[1:]
	


all_texts=[]
for i in train_texts1:
  all_texts.append(i)



for i in train_texts2:
  all_texts.append(i)



for i in test_texts1:
  all_texts.append(i)



for i in test_texts2:
  all_texts.append(i)
  


MAX_SEQUENCE_LENGTH = max_sentence_length
MAX_NB_WORDS = top_words_considered
EMBEDDING_DIM = word_vector_length
VALIDATION_SPLIT = validation_split
tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(all_texts)
train_sequences1 = tokenizer.texts_to_sequences(train_texts1)
train_sequences2 = tokenizer.texts_to_sequences(train_texts2)
test_sequences1 = tokenizer.texts_to_sequences(test_texts1)
test_sequences2 = tokenizer.texts_to_sequences(test_texts2)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
train_data1 = pad_sequences(train_sequences1, maxlen=MAX_SEQUENCE_LENGTH)
train_data2 = pad_sequences(train_sequences2, maxlen=MAX_SEQUENCE_LENGTH)
test_data1 = pad_sequences(test_sequences1, maxlen=MAX_SEQUENCE_LENGTH)
test_data2 = pad_sequences(test_sequences2, maxlen=MAX_SEQUENCE_LENGTH)
train_labels=[]
for i in train_y:
  train_labels.append(i)



train_labels=np.array(train_labels).reshape(len(train_y),1)
print('Shape of data tensor:', train_data1.shape)
print('Shape of data tensor:', train_data2.shape)
print('Shape of label tensor:', train_labels.shape)


print('Preparing embedding matrix.')
# prepare embedding matrix
nb_words = min(MAX_NB_WORDS, len(word_index))
embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))
for word, i in word_index.items():
  if i >= MAX_NB_WORDS:
    continue
  embedding_vector = embeddings_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector




# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
embedding_layer = Embedding(nb_words,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)



f=open('input/meta_ip.csv')
f=csv.reader(f)
meta_ip=[]

for i in f:
  meta_ip.append(float(i[0]))
  meta_ip.append(float(i[1]))
  meta_ip.append(float(i[2]))
  meta_ip.append(float(i[3]))
  meta_ip.append(float(i[4]))
  meta_ip.append(float(i[5]))
  meta_ip.append(float(i[6]))
  meta_ip.append(float(i[7]))
  meta_ip.append(float(i[8]))



meta_ip=np.array(meta_ip).reshape(len(nn_match),9)

input1_tensor = Input(train_data1.shape[1:])
input2_tensor = Input(train_data2.shape[1:])
input3_tensor = Input(meta_ip.shape[1:])

	
words_embedding_layer = embedding_layer
seq_embedding_layer = LSTM(1024, activation='sigmoid')
seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))
merge_layer = merge([seq_embedding(input1_tensor), seq_embedding(input2_tensor)])


dense1_layer = Dense(512, activation='sigmoid')(merge_layer)
d_test = Dense(512,activation='relu')(input3_tensor)
d_test = Dense(512,activation='sigmoid')(d_test)
final_merge_layer = merge([dense1_layer,d_test])

dense1_layer = Dense(512, activation='sigmoid')(final_merge_layer)
dense1_layer = Dropout(0.1)(dense1_layer)

dense2_layer = Dense(128, activation='sigmoid')(dense1_layer)
dense2_layer = Dropout(0.1)(dense2_layer)
dense3_layer = Dense(64, activation='sigmoid')(dense2_layer)
dense3_layer = Dropout(0.1)(dense3_layer)
dense4_layer = Dense(32, activation='sigmoid')(dense3_layer)
dense4_layer = Dropout(0.1)(dense4_layer)
dense5_layer = Dense(16, activation='sigmoid')(dense4_layer)
dense5_layer = Dropout(0.1)(dense5_layer)
dense6_layer = Dense(8, activation='sigmoid')(dense5_layer)
dense6_layer = Dropout(0.1)(dense6_layer)
dense7_layer = Dense(4, activation='sigmoid')(dense6_layer)
dense7_layer = Dropout(0.1)(dense7_layer)
output_layer = Dense(1, activation='sigmoid')(dense7_layer)


model2 = Model([input1_tensor, input2_tensor,input3_tensor], output_layer)

model2.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])
model2.summary()

model2.fit([train_data1, train_data2,meta_ip], train_y, batch_size=128, nb_epoch=15)





