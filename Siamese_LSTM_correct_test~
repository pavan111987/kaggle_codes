from __future__ import print_function
import os
import numpy as np
np.random.seed(1337)
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
# fix random seed for reproducibility
np.random.seed(7)
from keras.layers import merge
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
import sys
import csv


top_words_considered=10000
max_sentence_length=10
validation_split=0.4
file1='input/train.csv'
file2='input/test.csv'

word_vector_file='glove.6B.300d.txt'


print ("Loading word vectors.....")
embeddings_index = {}
f = open(word_vector_file)

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs


f.close()
print ("Word Vectors Loaded!!")
print('Found %s word vectors.' % len(embeddings_index))

word_vector_length=len(embeddings_index.get('dog'))


print ("Loading the data file....")
f=open(file1)
f=csv.reader(f)
train_texts1=[]
train_texts2=[]
x=[]
train_y=[]
for i in f:
  try:
    x.append(str(i[3]).encode('utf-8'))
    x.append(str(i[4]).encode('utf-8'))
    x.append(int(i[5]))
    train_texts1.append(str(i[3]).encode('utf-8'))
    train_texts2.append(str(i[4]).encode('utf-8'))
    train_y.append(int(i[5]))
  except:
    pass




test_texts1=[]
test_texts2=[]
f=open(file2)
f=csv.reader(f)
x=[]
for i in f:
  try:
    x.append(str(i[1]).encode('utf-8'))
    x.append(str(i[2]).encode('utf-8'))
    test_texts1.append(str(i[1]).encode('utf-8'))
    test_texts2.append(str(i[2]).encode('utf-8'))
  except:
    pass
 


test_texts1=test_texts1[1:]
test_texts2=test_texts2[1:]
	
all_texts=[]
for i in train_texts1:
  all_texts.append(i)


for i in train_texts2:
  all_texts.append(i)


for i in test_texts1:
  all_texts.append(i)

for i in test_texts2:
  all_texts.append(i)
  


MAX_SEQUENCE_LENGTH = max_sentence_length
MAX_NB_WORDS = top_words_considered
EMBEDDING_DIM = word_vector_length
VALIDATION_SPLIT = validation_split



tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(all_texts)
train_sequences1 = tokenizer.texts_to_sequences(train_texts1)
train_sequences2 = tokenizer.texts_to_sequences(train_texts2)

test_sequences1 = tokenizer.texts_to_sequences(test_texts1)
test_sequences2 = tokenizer.texts_to_sequences(test_texts2)


word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

train_data1 = pad_sequences(train_sequences1, maxlen=MAX_SEQUENCE_LENGTH)
train_data2 = pad_sequences(train_sequences2, maxlen=MAX_SEQUENCE_LENGTH)


test_data1 = pad_sequences(test_sequences1, maxlen=MAX_SEQUENCE_LENGTH)
test_data2 = pad_sequences(test_sequences2, maxlen=MAX_SEQUENCE_LENGTH)


train_labels=[]
for i in train_y:
  train_labels.append(i)


train_labels=np.array(train_labels).reshape(len(train_y),1)



print('Shape of data tensor:', train_data1.shape)
print('Shape of data tensor:', train_data2.shape)
print('Shape of label tensor:', train_labels.shape)


# split the data into a training set and a validation set
indices = np.arange(train_data1.shape[0])
np.random.shuffle(indices)
train_data1 = train_data1[indices]
train_data2 = train_data2[indices]
train_labels = train_labels[indices]
nb_validation_samples = int(VALIDATION_SPLIT * train_data1.shape[0])

x1_train = train_data1[:-nb_validation_samples]
x2_train = train_data2[:-nb_validation_samples]
y_train = train_labels[:-nb_validation_samples]
x1_val = train_data1[-nb_validation_samples:]
x2_val = train_data2[-nb_validation_samples:]
y_val = train_labels[-nb_validation_samples:]


print('Preparing embedding matrix.')

# prepare embedding matrix
nb_words = min(MAX_NB_WORDS, len(word_index))
embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))
for word, i in word_index.items():
  if i >= MAX_NB_WORDS:
    continue
  embedding_vector = embeddings_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector



# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
embedding_layer = Embedding(nb_words,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)

print('Training model.')


input1_tensor = Input(x1_train.shape[1:])
input2_tensor = Input(x2_train.shape[1:])

words_embedding_layer = embedding_layer
seq_embedding_layer = LSTM(512, activation='sigmoid')

seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))
merge_layer = merge([seq_embedding(input1_tensor), seq_embedding(input2_tensor)])

dense1_layer = Dense(16, activation='sigmoid')(merge_layer)
dense2_layer = Dense(16, activation='sigmoid')(dense1_layer)
output_layer = Dense(1, activation='sigmoid')(dense2_layer)


model = Model([input1_tensor, input2_tensor], output_layer)

model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])
model.summary()

model.fit([x1_train, x2_train], y_train, 
          validation_data=([x1_val, x2_val], y_val), 
          batch_size=128, nb_epoch=15)








